{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Img](https://app.theheadstarter.com/static/hs-logo-opengraph.png)\n",
        "\n",
        "# Headstarter RAG Workshop\n",
        "\n",
        "**Skills: OpenAI, LangChain, Pinecone**\n"
      ],
      "metadata": {
        "id": "4gQ6IlAfYJbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### What is RAG anyway?\n",
        "\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is a technique primarily used in GenAI applications to improve the quality and accuracy of generated text by LLMs by combining two key processes: retrieval and generation.\n",
        "\n",
        "### Breaking It Down:\n",
        "#### Retrieval:\n",
        "\n",
        "- Before generating a response, the system first looks up relevant information from a large database or knowledge base. This is like searching through a library or the internet to find the most useful facts, articles, or data related to the question or topic.\n",
        "\n",
        "#### Generation:\n",
        "\n",
        "- Once the relevant information is retrieved, the system then uses it to help generate a response. This is where the model, like GPT, creates new text (answers, explanations, etc.) based on the retrieved information."
      ],
      "metadata": {
        "id": "3kpEK0tGedpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install relevant libraries"
      ],
      "metadata": {
        "id": "UuyRz6Rhe7g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain langchain-community openai tiktoken pinecone-client langchain_pinecone unstructured pdfminer==20191125 pdfminer.six==20221105 pillow_heif unstructured_inference youtube-transcript-api pytube sentence-transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K6QAyJYEYx5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TBXE9kjoYHEF"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, WebBaseLoader, YoutubeLoader, DirectoryLoader, TextLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "import os\n",
        "\n",
        "pinecone_api_key = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ['PINECONE_API_KEY'] = pinecone_api_key\n",
        "\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the OpenAI client"
      ],
      "metadata": {
        "id": "XakMwUvciwvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "embed_model = \"text-embedding-3-small\"\n",
        "openai_client = OpenAI()"
      ],
      "metadata": {
        "id": "FNcULqsFYOP3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use HuggingFace & OpenRouter if you don't have an OpenAI account with credits\n",
        "\n"
      ],
      "metadata": {
        "id": "dneGEnGRbzIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace Embeddings\n",
        "# Use this instead of OpenAI embeddings if you don't have an OpenAI account with credits\n",
        "\n",
        "text = \"This is a test document.\"\n",
        "\n",
        "hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "query_result = hf_embeddings.embed_query(text)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Bsi17dZpacGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free Llama 3.1 API via OpenRouter\n",
        "# Use this instead of OpenAI if you don't have an OpenAI account with credits\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=userdata.get(\"OPENROUTER_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0xgG0zHfbwwf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize our text splitter\n",
        "This is how we will chunk up the text to be retrieved during the RAG process"
      ],
      "metadata": {
        "id": "seazzp38i0fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('p50k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=100,\n",
        "        length_function=tiktoken_len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z65sST2Vi2fY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Embeddings"
      ],
      "metadata": {
        "id": "_5nu6OEmfM_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    # Call the OpenAI API to get the embedding for the text\n",
        "    response = openai_client.embeddings.create(input=text, model=model)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def cosine_similarity_between_words(word1, word2):\n",
        "    # Get embeddings for both words\n",
        "    embedding1 = np.array(get_embedding(word1))\n",
        "    embedding2 = np.array(get_embedding(word2))\n",
        "\n",
        "    # Reshape embeddings for cosine_similarity function\n",
        "    embedding1 = embedding1.reshape(1, -1)\n",
        "    embedding2 = embedding2.reshape(1, -1)\n",
        "\n",
        "    print(\"Embedding for Word 1:\", embedding1)\n",
        "    print(\"\\nEmbedding for Word 2:\", embedding2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = cosine_similarity(embedding1, embedding2)\n",
        "    return similarity[0][0]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "word1 = \"Great to finally meet!\"\n",
        "word2 = \"Nice to meet you\"\n",
        "\n",
        "\n",
        "similarity = cosine_similarity_between_words(word1, word2)\n",
        "print(f\"\\n\\nCosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXYyWLfPIZ9g",
        "outputId": "fc6d0104-82c7-4eed-e32d-dfa1fdda98ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for Word 1: [[-0.00409901 -0.02935333 -0.06173278 ...  0.0112379  -0.00698414\n",
            "  -0.00348519]]\n",
            "\n",
            "Embedding for Word 2: [[-0.00058948 -0.05813902 -0.06282136 ...  0.01669302 -0.01365682\n",
            "  -0.01385192]]\n",
            "\n",
            "\n",
            "Cosine similarity between 'Great to finally meet!' and 'Nice to meet you': 0.6680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_FJwp-9MYOSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in a YouTube video and get its transcript"
      ],
      "metadata": {
        "id": "GKfmWW3LikeH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3bk4AiyHOdrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize Pinecone"
      ],
      "metadata": {
        "id": "uEUmcsiRO5Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = PineconeVectorStore(index_name=\"\", embedding=embeddings)\n",
        "\n",
        "index_name = \"\"\n",
        "\n",
        "namespace = \"\""
      ],
      "metadata": {
        "id": "fszB8F8VO6y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbNg2vaSPCPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insert data into Pinecone"
      ],
      "metadata": {
        "id": "irzuVetUPPoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentation: https://docs.pinecone.io/integrations/langchain#key-concepts"
      ],
      "metadata": {
        "id": "4KthyyMXZlOY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a3fwGOUnQIO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GMPp1oKLjlCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform RAG"
      ],
      "metadata": {
        "id": "KNiqo6CK6VRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone"
      ],
      "metadata": {
        "id": "dzNO18CUAyLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=userdata.get(\"PINECONE_API_KEY\"),)\n",
        "\n",
        "# Connect to your Pinecone index\n",
        "pinecone_index = pc.Index(\"\")"
      ],
      "metadata": {
        "id": "Vx4slePeAt_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3cyFmjCl6wcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2P8qv2ThFOgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together"
      ],
      "metadata": {
        "id": "sNaoskKIXY6b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSofnv8WfffH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hI7Zg4yrffh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cFmMHL6XXR5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG over a PDF"
      ],
      "metadata": {
        "id": "6y73jpVCYimh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/Harry Potter and the Sorcerers Stone.pdf\")\n",
        "data = loader.load()\n",
        "\n",
        "print(data)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=100,\n",
        "        length_function=tiktoken_len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "texts = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "B7COTIz4Yka9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vXnygYkvlu47"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}